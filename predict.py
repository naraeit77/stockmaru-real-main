# Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
import subprocess
import sys
import os
import time
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

try:
    from supabase import create_client, Client
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "supabase"])
    from supabase import create_client, Client
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D
)
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import json
import pickle
from sklearn.metrics import mean_absolute_error, mean_squared_error

# í•˜ë“œì›¨ì–´ ê°€ì† ì„¤ì •
print("=" * 50)
print("í•˜ë“œì›¨ì–´ ê°€ì† ì„¤ì •")
print("=" * 50)

# CPU ìµœì í™” ì„¤ì •
tf.config.threading.set_intra_op_parallelism_threads(0)  # ìë™ ì„¤ì •
tf.config.threading.set_inter_op_parallelism_threads(0)  # ìë™ ì„¤ì •

# GPU/Metal ê°ì§€
gpus = tf.config.list_physical_devices('GPU')
all_devices = tf.config.list_physical_devices()

print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {[d.device_type for d in all_devices]}")

if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpus)}ê°œ GPU ê°ì§€")
        # Mixed Precision í™œì„±í™” (GPU ì„±ëŠ¥ í–¥ìƒ)
        tf.keras.mixed_precision.set_global_policy('mixed_float16')
        print("âœ… Mixed Precision (FP16) í™œì„±í™”")
    except RuntimeError as e:
        print(f"âš ï¸  GPU ì„¤ì • ì˜¤ë¥˜: {e}")
        gpus = None
else:
    print("â„¹ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
    print("ğŸ’¡ Macì—ì„œ GPU ê°€ì†ì„ ì›í•˜ì‹œë©´ TensorFlow 2.13-2.15 ë²„ì „ê³¼ tensorflow-metalì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("   pip uninstall tensorflow")
    print("   pip install tensorflow==2.15.0 tensorflow-metal")

    # CPU ìµœì í™” í™œì„±í™”
    import os as os_env
    os_env.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'
    print("âœ… CPU ìµœì í™” í™œì„±í™” (oneDNN)")

print("=" * 50)

# Supabase ì—°ê²° ì„¤ì • (.env íŒŒì¼ì—ì„œ ì½ê¸°)
url: str = os.getenv("SUPABASE_URL", "")
key: str = os.getenv("SUPABASE_KEY", "")

if not url or not key:
    raise ValueError("SUPABASE_URLê³¼ SUPABASE_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

supabase: Client = create_client(url, key)

# Supabaseì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
# def get_stock_data_from_db():
#     try:
#         response = supabase.table("economic_and_stock_data").select("*").order("ë‚ ì§œ", desc=False).execute()
#         print(f"economic_and_stock_data í…Œì´ë¸”ì—ì„œ {len(response.data)}ê°œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")
#         print(response.data)
#         # ì‘ë‹µ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
#         df = pd.DataFrame(response.data)

#         # ë‚ ì§œ ì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
#         df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
#         df.sort_values(by='ë‚ ì§œ', inplace=True)

#         print("Handling missing values and filtering invalid data...")
#         df.fillna(method='ffill', inplace=True)
#         df.fillna(method='bfill', inplace=True)
#         df = df.apply(pd.to_numeric, errors='coerce')
#         df.dropna(inplace=True)

#         return df
#     except Exception as e:
#         print(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
#         return None

def get_stock_data_from_db():
    try:
        # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_data = get_all_data("economic_and_stock_data")
        print(f"economic_and_stock_data í…Œì´ë¸”ì—ì„œ {len(all_data)}ê°œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")
        df = pd.DataFrame(all_data)

        # ë‚ ì§œ ì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
        df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
        df.sort_values(by='ë‚ ì§œ', inplace=True)

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        print("ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
        df = df.ffill().bfill()  # ì•/ë’¤ ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
        exclude_columns = ['ë‚ ì§œ']
        numeric_columns = [col for col in df.columns if col not in exclude_columns]
        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')

        # NaN ë¹„ìœ¨ í™•ì¸
        nan_ratios = df[numeric_columns].isna().mean()
        print("ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë³„ NaN ë¹„ìœ¨:")
        print(nan_ratios)

        # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ dropna ëŒ€ìƒìœ¼ë¡œ ì„¤ì •
        valid_columns = [col for col in numeric_columns if nan_ratios[col] < 1.0]
        df.dropna(subset=valid_columns, inplace=True)

        print(f"ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {df.shape}")
        return df
    except Exception as e:
        print(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return None

def get_all_data(table_name, use_cache=True):
    """
    Supabaseì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìºì‹± ì§€ì›)

    Args:
        table_name: í…Œì´ë¸” ì´ë¦„
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
    """
    cache_file = f"{table_name}_cache.pkl"

    # ìºì‹œ íŒŒì¼ì´ ìˆê³  24ì‹œê°„ ì´ë‚´ë©´ ìºì‹œ ì‚¬ìš©
    if use_cache and os.path.exists(cache_file):
        cache_age = time.time() - os.path.getmtime(cache_file)
        if cache_age < 86400:  # 24ì‹œê°„ = 86400ì´ˆ
            print(f"ìºì‹œëœ ë°ì´í„° ì‚¬ìš© (ìºì‹œ ë‚˜ì´: {cache_age/3600:.1f}ì‹œê°„)")
            with open(cache_file, 'rb') as f:
                return pickle.load(f)

    print(f"{table_name} í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘...")
    all_data = []
    offset = 0
    limit = 1000  # Supabaseì˜ ê¸°ë³¸ ì œí•œ

    while True:
        response = supabase.table(table_name).select("*").order("ë‚ ì§œ", desc=False).limit(limit).offset(offset).execute()
        data = response.data
        if not data:  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            break
        all_data.extend(data)
        offset += limit
        print(f"  {len(all_data)}ê°œ ë¡œë“œë¨...", end='\r')

    print(f"  ì´ {len(all_data)}ê°œ ë¡œë“œ ì™„ë£Œ")

    # ìºì‹œ ì €ì¥
    if use_cache:
        with open(cache_file, 'wb') as f:
            pickle.dump(all_data, f)
        print(f"ìºì‹œ íŒŒì¼ ì €ì¥: {cache_file}")

    return all_data

# Transformer Encoder ì •ì˜
def transformer_encoder(inputs, num_heads, ff_dim, dropout=0.1):
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)
    attention_output = Dropout(dropout)(attention_output)
    attention_output = Add()([inputs, attention_output])
    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)

    ffn = Dense(ff_dim, activation="relu")(attention_output)
    ffn = Dense(inputs.shape[-1])(ffn)
    ffn_output = Dropout(dropout)(ffn)
    ffn_output = Add()([attention_output, ffn_output])
    ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output)

    return ffn_output

# Transformer ëª¨ë¸ ì •ì˜
def build_transformer_with_two_inputs(stock_shape, econ_shape, num_heads, ff_dim, target_size):
    stock_inputs = Input(shape=stock_shape)
    stock_encoded = stock_inputs
    for _ in range(4):  # 4ê°œì˜ Transformer Layer
        stock_encoded = transformer_encoder(stock_encoded, num_heads=num_heads, ff_dim=ff_dim)
    stock_encoded = Dense(64, activation="relu")(stock_encoded)

    econ_inputs = Input(shape=econ_shape)
    econ_encoded = econ_inputs
    for _ in range(4):  # 4ê°œì˜ Transformer Layer
        econ_encoded = transformer_encoder(econ_encoded, num_heads=num_heads, ff_dim=ff_dim)
    econ_encoded = Dense(64, activation="relu")(econ_encoded)

    merged = Add()([stock_encoded, econ_encoded])
    merged = Dense(128, activation="relu")(merged)
    merged = Dropout(0.2)(merged)
    merged = GlobalAveragePooling1D()(merged)

    # Mixed Precision ì‚¬ìš© ì‹œ ì¶œë ¥ ë ˆì´ì–´ëŠ” float32ë¡œ ì„¤ì •
    outputs = Dense(target_size, dtype='float32')(merged)

    return Model(inputs=[stock_inputs, econ_inputs], outputs=outputs)

print("Loading data from database...")
data = get_stock_data_from_db()
if data is None or data.empty:
    raise ValueError("DBì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")

# data.sort_values(by='ë‚ ì§œ', inplace=True)

# print("Handling missing values and filtering invalid data...")
# data.fillna(method='ffill', inplace=True)
# data.fillna(method='bfill', inplace=True)
# data = data.apply(pd.to_numeric, errors='coerce')
# data.dropna(inplace=True)

forecast_horizon = 14  # ì˜ˆì¸¡ ê¸°ê°„ (14ì¼ í›„ë¥¼ ì˜ˆì¸¡)

target_columns = [
    'ì• í”Œ', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'ì•„ë§ˆì¡´', 'êµ¬ê¸€ A', 'êµ¬ê¸€ C', 'ë©”íƒ€',
    'í…ŒìŠ¬ë¼', 'ì—”ë¹„ë””ì•„', 'ì½”ìŠ¤íŠ¸ì½”', 'ë„·í”Œë¦­ìŠ¤', 'í˜ì´íŒ”', 'ì¸í…”', 'ì‹œìŠ¤ì½”', 'ì»´ìºìŠ¤íŠ¸',
    'í©ì‹œì½”', 'ì•”ì  ', 'í—ˆë‹ˆì›° ì¸í„°ë‚´ì…”ë„', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ëª¬ë¸ë¦¬ì¦ˆ', 'ë§ˆì´í¬ë¡ ', 'ë¸Œë¡œë“œì»´',
    'ì–´ë„ë¹„', 'í…ì‚¬ìŠ¤ ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸', 'AMD', 'ì–´í”Œë¼ì´ë“œ ë¨¸í‹°ë¦¬ì–¼ì¦ˆ', 'S&P 500 ETF', 'QQQ ETF', 'string'
]

economic_features = [
    '10ë…„ ê¸°ëŒ€ ì¸í”Œë ˆì´ì…˜ìœ¨', 'ì¥ë‹¨ê¸° ê¸ˆë¦¬ì°¨', 'ê¸°ì¤€ê¸ˆë¦¬', 'ë¯¸ì‹œê°„ëŒ€ ì†Œë¹„ì ì‹¬ë¦¬ì§€ìˆ˜',
    'ì‹¤ì—…ë¥ ', '2ë…„ ë§Œê¸° ë¯¸êµ­ êµ­ì±„ ìˆ˜ìµë¥ ', '10ë…„ ë§Œê¸° ë¯¸êµ­ êµ­ì±„ ìˆ˜ìµë¥ ', 'ê¸ˆìœµìŠ¤íŠ¸ë ˆìŠ¤ì§€ìˆ˜',
    'ê°œì¸ ì†Œë¹„ ì§€ì¶œ', 'ì†Œë¹„ì ë¬¼ê°€ì§€ìˆ˜', '5ë…„ ë³€ë™ê¸ˆë¦¬ ëª¨ê¸°ì§€', 'ë¯¸êµ­ ë‹¬ëŸ¬ í™˜ìœ¨',
    'í†µí™” ê³µê¸‰ëŸ‰ M2', 'ê°€ê³„ ë¶€ì±„ ë¹„ìœ¨', 'GDP ì„±ì¥ë¥ ', 'ë‚˜ìŠ¤ë‹¥ ì¢…í•©ì§€ìˆ˜', 'S&P 500 ì§€ìˆ˜', 'ê¸ˆ ê°€ê²©', 'ë‹¬ëŸ¬ ì¸ë±ìŠ¤', 'ë‚˜ìŠ¤ë‹¥ 100',
    'S&P 500 ETF', 'QQQ ETF', 'ëŸ¬ì…€ 2000 ETF', 'ë‹¤ìš° ì¡´ìŠ¤ ETF', 'VIX ì§€ìˆ˜',
    'ë‹›ì¼€ì´ 225', 'ìƒí•´ì¢…í•©', 'í•­ì…', 'ì˜êµ­ FTSE', 'ë…ì¼ DAX', 'í”„ë‘ìŠ¤ CAC 40',
    'ë¯¸êµ­ ì „ì²´ ì±„ê¶Œì‹œì¥ ETF', 'TIPS ETF', 'íˆ¬ìë“±ê¸‰ íšŒì‚¬ì±„ ETF', 'ë‹¬ëŸ¬/ì—”', 'ë‹¬ëŸ¬/ìœ„ì•ˆ',
    'ë¯¸êµ­ ë¦¬ì¸  ETF'
]

print("Scaling data...")
train_size = int(len(data) * 0.8)
train_data = data.iloc[:train_size]
test_data = data.iloc[train_size:]

data_scaled = data.copy()
stock_scaler = MinMaxScaler()
econ_scaler = MinMaxScaler()

data_scaled[target_columns] = stock_scaler.fit_transform(data[target_columns])
data_scaled[economic_features] = econ_scaler.fit_transform(data[economic_features])

lookback = 90

# í›ˆë ¨ ë°ì´í„° ìƒì„±
X_stock_train = []
X_econ_train = []
y_train = []

for i in range(lookback, len(data_scaled) - forecast_horizon):
    X_stock_seq = data_scaled[target_columns].iloc[i - lookback:i].to_numpy()
    X_econ_seq = data_scaled[economic_features].iloc[i - lookback:i].to_numpy()
    y_val = data_scaled[target_columns].iloc[i + forecast_horizon - 1].to_numpy()
    X_stock_train.append(X_stock_seq)
    X_econ_train.append(X_econ_seq)
    y_train.append(y_val)

X_stock_train = np.array(X_stock_train)
X_econ_train = np.array(X_econ_train)
y_train = np.array(y_train)

# ì „ì²´ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±: ë§ˆì§€ë§‰ ë‚ ì§œê¹Œì§€ í¬í•¨í•˜ì—¬ ì˜ˆì¸¡ (ë¯¸ë˜ ì‹¤ì œê°’ ì—†ì–´ë„ ì˜ˆì¸¡)
X_stock_full = []
X_econ_full = []
for i in range(lookback, len(data_scaled)):  # ì—¬ê¸°ì„œ forecast_horizon ë¹¼ì§€ ì•ŠìŒ
    X_stock_seq = data_scaled[target_columns].iloc[i - lookback:i].to_numpy()
    X_econ_seq = data_scaled[economic_features].iloc[i - lookback:i].to_numpy()
    X_stock_full.append(X_stock_seq)
    X_econ_full.append(X_econ_seq)

X_stock_full = np.array(X_stock_full)
X_econ_full = np.array(X_econ_full)

print("Building Transformer model...")
stock_shape = (lookback, len(target_columns))
econ_shape = (lookback, len(economic_features))

model = build_transformer_with_two_inputs(stock_shape, econ_shape, num_heads=8, ff_dim=256, target_size=len(target_columns))
model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])
model.summary()

print("Training model...")

# ì½œë°± ì„¤ì •
callbacks = [
    # Early Stopping: ê²€ì¦ ì†ì‹¤ì´ 10 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
    EarlyStopping(
        monitor='loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    # Model Checkpoint: ìµœìƒì˜ ëª¨ë¸ ì €ì¥
    ModelCheckpoint(
        'best_stock_model.keras',
        monitor='loss',
        save_best_only=True,
        verbose=1
    ),
    # Learning Rate Reduction: í•™ìŠµì´ ì •ì²´ë˜ë©´ í•™ìŠµë¥  ê°ì†Œ
    ReduceLROnPlateau(
        monitor='loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    )
]

# ë°°ì¹˜ í¬ê¸°ë¥¼ 64ë¡œ ì¦ê°€ (GPU ì‚¬ìš© ì‹œ ì„±ëŠ¥ í–¥ìƒ)
batch_size = 64 if gpus else 32

history = model.fit(
    [X_stock_train, X_econ_train],
    y_train,
    epochs=50,
    batch_size=batch_size,
    callbacks=callbacks,
    verbose=1
)

print("Performing full predictions...")
predicted_prices = model.predict([X_stock_full, X_econ_full], verbose=1)
predicted_prices_actual = stock_scaler.inverse_transform(predicted_prices)

pred_len = len(predicted_prices_actual)

# ì˜¤ëŠ˜ ë‚ ì§œë“¤ (ë§ˆì§€ë§‰ ë‚ ì§œê¹Œì§€ í¬í•¨)
today_dates = data['ë‚ ì§œ'].iloc[lookback : lookback + pred_len].values

# ì˜¤ëŠ˜ ì‹¤ì œ ì£¼ê°€ (ì˜¤ëŠ˜ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œê°’), ë°ì´í„° ë²”ìœ„ ë„˜ì–´ê°€ë©´ NaN ì²˜ë¦¬
actual_data_end = min(lookback + pred_len, len(data))
actual_full = data[target_columns].iloc[lookback:actual_data_end].values

# ë§Œì•½ actual_full ê¸¸ì´ê°€ pred_lenë³´ë‹¤ ì§§ë‹¤ë©´ ë¶€ì¡±í•œ ë¶€ë¶„ì„ NaNìœ¼ë¡œ ì±„ì›€
if actual_full.shape[0] < pred_len:
    nan_padding = np.full((pred_len - actual_full.shape[0], len(target_columns)), np.nan)
    actual_full = np.vstack([actual_full, nan_padding])

result_data = pd.DataFrame({'ë‚ ì§œ': today_dates})

for idx, col in enumerate(target_columns):
    result_data[f'{col}_Predicted'] = predicted_prices_actual[:, idx]
    result_data[f'{col}_Actual'] = actual_full[:, idx]

result_data['ë‚ ì§œ'] = pd.to_datetime(result_data['ë‚ ì§œ'], errors='coerce')
result_data['ë‚ ì§œ'] = result_data['ë‚ ì§œ'].dt.strftime('%Y-%m-%d')

# # ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥
def save_predictions_to_db(result_df):
    try:
        # ê¸°ì¡´ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„± (predicted_stocks í…Œì´ë¸”ì— ì €ì¥)
        records = result_df.to_dict('records')

        # í…Œì´ë¸”ì— ë¨¼ì € ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
        supabase.table("predicted_stocks").delete().neq("id", 0).execute()

        # ì¼ê´„ ì‚½ì… (í° ë°ì´í„°ë¼ë©´ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì‚½ì…)
        chunk_size = 100
        for i in range(0, len(records), chunk_size):
            chunk = records[i:i+chunk_size]
            response = supabase.table("predicted_stocks").insert(chunk).execute()

        print(f"{len(records)}ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")

# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
save_predictions_to_db(result_data)

plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

for col in target_columns:
    plt.figure(figsize=(12, 6))
    plt.plot(pd.to_datetime(result_data['ë‚ ì§œ']), result_data[f'{col}_Actual'], label='Actual (Today)', alpha=0.7)
    plt.plot(pd.to_datetime(result_data['ë‚ ì§œ']), result_data[f'{col}_Predicted'], label=f'Predicted ({forecast_horizon} days later)', alpha=0.7)
    plt.title(f'{col} - Actual(Today) vs Predicted({forecast_horizon} days later)')
    plt.xlabel('Date (Today)')
    plt.ylabel('Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid()
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gcf().autofmt_xdate()
    plt.close()

print(f"ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

#################################### ê²°ê³¼ ì¶”ë¡  ####################################
#################################### ê²°ê³¼ ì¶”ë¡  ####################################
#################################### ê²°ê³¼ ì¶”ë¡  ####################################

######################
# (0) Get Predictions From DB Function
######################

# Supabaseì—ì„œ ì˜ˆì¸¡ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì²­í¬ ë‹¨ìœ„)
def get_predictions_from_db(chunk_size=1000):
    try:
        # ì „ì²´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        count_response = supabase.table("predicted_stocks").select("id", count="exact").execute()
        total_count = count_response.count
        print(f"predicted_stocks í…Œì´ë¸”ì˜ ì´ ë ˆì½”ë“œ ìˆ˜: {total_count}")

        # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸
        all_data = []

        # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        for offset in range(0, total_count, chunk_size):
            response = (
                supabase.table("predicted_stocks")
                .select("*")
                .order("ë‚ ì§œ", desc=False)
                .limit(chunk_size)
                .offset(offset)
                .execute()
            )
            chunk_data = response.data
            print(f"ì˜¤í”„ì…‹ {offset}ì—ì„œ {len(chunk_data)}ê°œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            all_data.extend(chunk_data)

        # ëª¨ë“  ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(all_data)
        print(f"ì´ {len(df)}ê°œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")

        # ë‚ ì§œ ì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])

        return df
    except Exception as e:
        print(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return None

# ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥
def save_analysis_to_db(result_df):
    try:
        # stock_analysis_results í…Œì´ë¸”ì— ì €ì¥
        records = result_df.to_dict('records')

        # í…Œì´ë¸”ì— ë¨¼ì € ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
        supabase.table("stock_analysis_results").delete().neq("id", 0).execute()

        # ì¼ê´„ ì‚½ì… (í° ë°ì´í„°ë¼ë©´ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì‚½ì…)
        chunk_size = 100
        for i in range(0, len(records), chunk_size):
            chunk = records[i:i+chunk_size]
            response = supabase.table("stock_analysis_results").insert(chunk).execute()

        print(f"{len(records)}ê°œì˜ ë¶„ì„ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")

######################
# (1) Evaluation Function
######################
def evaluate_predictions(data, target_columns, forecast_horizon):
    """
    This function compares actual vs. predicted values (for the next 7 days)
    and computes various metrics such as MAE, MSE, RMSE, MAPE, and Accuracy.

    - MAE (Mean Absolute Error): Average absolute error between actual and predicted
      (lower is better, same unit as original data)
    - MSE (Mean Squared Error): Average of squared errors
      (lower is better)
    - RMSE (Root Mean Squared Error): Square root of MSE
      (lower is better, often used with MAE)
    - MAPE (Mean Absolute Percentage Error): Error as a percentage of the actual values
      (lower is better)
    - Accuracy (%): Computed as 100 - MAPE, serving as a simple accuracy measure
    """

    metrics = []

    for col in target_columns:
        # ì›ë˜ ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
        predicted_col = f'{col}_Predicted'
        actual_col = f'{col}_Actual'

        # Check if the columns exist
        if predicted_col not in data.columns or actual_col not in data.columns:
            print(f"Skipping {col}: Columns not found in data ({predicted_col}, {actual_col})")
            continue

        # Retrieve predicted and actual values
        predicted = data[predicted_col]
        # Shift the actual values by forecast_horizon days
        # so that today's prediction aligns with actual values 14 days ahead
        actual = data[actual_col].shift(-forecast_horizon)

        # Use only valid (non-NaN) indices
        valid_idx = ~predicted.isna() & ~actual.isna()
        predicted = predicted[valid_idx]
        actual = actual[valid_idx]

        if len(predicted) == 0:
            print(f"Skipping {col}: No valid prediction/actual pairs.")
            continue

        # Calculate metrics
        mae = mean_absolute_error(actual, predicted)
        mse = mean_squared_error(actual, predicted)
        rmse = mse ** 0.5
        mape = (abs((actual - predicted) / actual).mean()) * 100
        accuracy = 100 - mape

        metrics.append({
            'Stock': col,
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'MAPE (%)': mape,
            'Accuracy (%)': accuracy
        })

    return pd.DataFrame(metrics)

###############################
# (2) Future Rise Analysis
###############################
def analyze_rise_predictions(data, target_columns):
    """
    This function looks at the last row of the DataFrame (most recent date),
    compares actual vs. predicted values, and calculates rise/fall information
    and rise probability in percentage.
    """

    last_row = data.iloc[-1]
    results = []

    for col in target_columns:
        # ì›ë˜ ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
        actual_col = f'{col}_Actual'
        predicted_col = f'{col}_Predicted'

        last_actual_price = last_row.get(actual_col, np.nan)
        predicted_future_price = last_row.get(predicted_col, np.nan)

        # Determine rise/fall and rise percentage
        if pd.notna(last_actual_price) and pd.notna(predicted_future_price):
            predicted_rise = predicted_future_price > last_actual_price
            rise_probability = ((predicted_future_price - last_actual_price) / last_actual_price) * 100
        else:
            predicted_rise = np.nan
            rise_probability = np.nan

        results.append({
            'Stock': col,
            'Last Actual Price': last_actual_price,
            'Predicted Future Price': predicted_future_price,
            'Predicted Rise': predicted_rise,
            'Rise Probability (%)': rise_probability
        })

    return pd.DataFrame(results)

#######################################
# (3) Buy/Sell Recommendation and Analysis
#######################################
def generate_recommendation(row):
    """
    Example logic:
    - (Predicted Rise == True) and (Rise Probability > 0) => BUY
    - (Rise Probability > 2) => STRONG BUY
    - Otherwise => SELL
    """
    rise_prob = row.get('Rise Probability (%)', 0)
    predicted_rise = row.get('Predicted Rise', False)

    if pd.isna(rise_prob) or pd.isna(predicted_rise):
        return "No Data"

    if predicted_rise and rise_prob > 0:
        if rise_prob > 2:
            return "STRONG BUY"
        else:
            return "BUY"
    else:
        return "SELL"

def generate_analysis(row):
    """
    Provides a one-line comment for each entry.
    Stock: stock name
    Rise Probability (%): approximate rise probability
    """
    stock_name = row['Stock']
    rise_prob = row.get('Rise Probability (%)', 0)
    predicted_rise = row.get('Predicted Rise', False)

    if pd.isna(rise_prob) or pd.isna(predicted_rise):
        return f"{stock_name}: Not enough data"

    if predicted_rise:
        return f"{stock_name} is expected to rise by about {rise_prob:.2f}%. Consider buying or holding."
    else:
        return f"{stock_name} is expected to fall by about {-rise_prob:.2f}%. A cautious approach is recommended."

#######################
# (4) Main Code
#######################
# 1) Load Data from Supabase
data = get_predictions_from_db(chunk_size=1000)
if data is None or len(data) == 0:
    print("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    exit(1)

# 2) Target columns
target_columns = [
    'ì• í”Œ', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'ì•„ë§ˆì¡´', 'êµ¬ê¸€ A', 'êµ¬ê¸€ C', 'ë©”íƒ€',
    'í…ŒìŠ¬ë¼', 'ì—”ë¹„ë””ì•„', 'ì½”ìŠ¤íŠ¸ì½”', 'ë„·í”Œë¦­ìŠ¤', 'í˜ì´íŒ”', 'ì¸í…”', 'ì‹œìŠ¤ì½”', 'ì»´ìºìŠ¤íŠ¸',
    'í©ì‹œì½”', 'ì•”ì  ', 'í—ˆë‹ˆì›° ì¸í„°ë‚´ì…”ë„', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ëª¬ë¸ë¦¬ì¦ˆ', 'ë§ˆì´í¬ë¡ ', 'ë¸Œë¡œë“œì»´',
    'ì–´ë„ë¹„', 'í…ì‚¬ìŠ¤ ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸', 'AMD', 'ì–´í”Œë¼ì´ë“œ ë¨¸í‹°ë¦¬ì–¼ì¦ˆ', 'S&P 500 ETF', 'QQQ ETF'
]

forecast_horizon = 14  # predicting 14 days ahead

# 3) Evaluate predictions
evaluation_results = evaluate_predictions(data, target_columns, forecast_horizon)
print("============ Evaluation Results ============")
print(evaluation_results)

# 4) Analyze future rise
rise_results = analyze_rise_predictions(data, target_columns)
print("============ Rise Predictions ============")
print(rise_results)

# 5) Merge DataFrames (evaluation metrics + rise analysis)
final_results = pd.merge(evaluation_results, rise_results, on='Stock', how='outer')

# 6) Sort by rise probability (descending order)
final_results = final_results.sort_values(by='Rise Probability (%)', ascending=False)

# 7) Generate buy/sell recommendations and analysis
final_results['Recommendation'] = final_results.apply(generate_recommendation, axis=1)
final_results['Analysis'] = final_results.apply(generate_analysis, axis=1)

# Reorder columns
column_order = [
    'Stock',
    'MAE', 'MSE', 'RMSE', 'MAPE (%)', 'Accuracy (%)',
    'Last Actual Price', 'Predicted Future Price', 'Predicted Rise', 'Rise Probability (%)',
    'Recommendation', 'Analysis'
]
final_results = final_results[column_order]

# 8) Save final results to Supabase
save_analysis_to_db(final_results)
print("\në¶„ì„ ê²°ê³¼ê°€ 'stock_analysis_results' í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 9) Print final report
print("=============== Final Report ===============")
print(final_results.to_string(index=False))
